%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{CMPUT 497 Project Draft Report: \\ RAKE - Key Word Extraction Replication}

\author{Shouyang Zhou \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt shoyang@ualberta.ca} \\\And
  Sharon Hains \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt hains@ualberta.ca} \\\And
  Sharif Bakouny \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt albakoun@ualberta.ca} \\}


\date{}

\begin{document}
\maketitle

\section{Introduction}

We aim to replicate a 2010 paper detailing a text mining method titled RAKE – Rapid Automatic Keyword Extraction. Keyword extraction is the automated process of extracting important words and phrases from a document. This paper devises a new method for keyword extraction and compares the method to a baseline method in two circumstances, using scientific abstracts, and news articles. The importance of keyword extraction is shown over its wide domain of application. It has many applications in information retrieval, feature engineering, and assistive technologies.  It is also applied in search engines and classification technologies. At its most basic level, it helps humans process unstructured data in a more efficient and digestible manner. The importance of this replication study is to verify the results of \citet{1}, so we can be more confident in using RAKE as keyword extraction method. Confirming the results of RAKE testing is relevant as keyword extraction is a widely used application as discussed above. 

We will be using the dataset used in RAKE as our input, which is a set of abstracts, and processing the data. Our output will be a generation of keywords extracted from this set of abstracts. We will be comparing the number of correct keywords to extracted keywords, to determine how accurate our replication is to the original paper. 

\section{Related Work}

The original paper “Automatic Keyword Extraction from Individual Documents” by \citet{1} is the primary work of interest. As mentioned previously, they demonstrate their method and compare it with TextRank on two datasets. We note that there are existing implementations of RAKE and TextRank in Python 3 (\citet{2}, \citet{3}).

RAKE stands for 'Rapid Automatic Keyword Extraction', and extracts keywords based on the occurence of certain words \citep{1}. The way it works is that content words, a set of phrase and word delimiters, and list of stop words are taken in. Stop words are dropped from being included in the content words, as they are usually uninformative or meaningless. Stop words can include punctuation, numbers, conjuctions, and more. 

To implement the RAKE algorithm, it goes as follows:
\begin{enumerate}
\item Split the text into an array of words using the word delimiters.
\item Split the array into sequences contiguous words using stop words and phrase delimiters.
\item Candidate keywords are words in a sequence that are assigned the same position in the text. 
\item Assign scores to each keyword candidate using ration of degree to frequency. 
\item Keywords that contain stop words:
\begin{enumerate}
\item A pair of candidate keywords must be adjoind at least twice in the text in the same order.
\item Create a new keyword which contains the pair of keywords with interior stopwords between them.
\item The new keyword’s score is the sum of the scores of its keywords components.
\end{enumerate}
\item The keywords of the text are the top T keywords from the keyword candidates list.
\end{enumerate}

\citet{1} also develops a method for stoplist generation, where they use document frequency, keyword frequency (how many times the word appears in the keywords set) and adjacency frequency (how many times the word appears adjacent to a keyword). Using only term frequency risks adding content bearing words to the stoplist, so adjacency frequency was used. However, words that appear more often in the keywords list than adjacent to keywords are excluded from the stoplist.   

In evaluating RAKE, no training sets are used because RAKE is unsupervised learning algorithm. Precision, Recall and F-Measure were used for evaluating the results. The results of RAKE were compared with the results of testing the same set of texts using TextRank and Supervised Learning (Hulth 2003). Perfect precision is not possible with any of the techniques because some of the manually assigned keywords might not appear in the text. The performance of RAKE depends on the stoplist that was used. The best precision, F-measure and a comparable recall can be achieved by using the keyword adjacency stoplist. Using Fox’s stoplist yields high recall and low precision. RAKE outperforms all previously used keyword extraction algorithms in precision, efficiency and simplicity.

To go into more detail about TextRank, \citet{4} discusses how TextRank is a graph-based ranking algorithm, where the importance of a vertex is decided by considering global information recursively computed from the entire graph. Implementing this algorithm goes as follows:

\begin{enumerate}
\item Identify text units and add them as vertices to the graph.
\item Edges of the graph are relations between text units.
\item Iterate the algorithm until convergence below a given threshold. 
\item Rank vertices based on their final scores (values).
\end{enumerate}   

Evaluating TextRank, the results were compared with the uncontrolled set of keywords (Freely assigned by the indexers and unrestricted to a given dictionary). Precision, recall and F-measure were used for evaluation. Best performance is achieved with smaller window, a filter that only considers Nouns and Adjectives when pre-processing the text, and when the testing was on undirected graphs (no direction of the relation between the text units). The results obtained by TextRank were higher than all previously proposed systems. However, RAKE algorithm was not used and the results were not compared to RAKE’s results.

We will not be recreating the results of \citet{hulth-2003-improved}, however as we are including their results in our evaluation, we would like to note their method. \citet{1} describe their method where "Hulth (2003) compares the effectiveness of three term selection approaches: noun-phrase (NP) chunks, n-grams, and POS tags, with four discriminative features of these terms as inputs for automatic keyword extraction using a supervised machine-learning algorithm."    

\section{Implementation}

We aim to implement RAKE and Textrank using existing Python modules. We will then create an evaluation framework to reproduce the results from the paper. For example, the first evaluation \citet{1} proposes compares RAKE and TextRank variants on a corpus of scientific abstracts, comparing accuracy as a measure of precision and recall over 500 abstracts. In this case, our implementation would be using the existing Python implementation of the RAKE algorithm, and an aggregation process to feed in and mark individual abstracts and in the end summarize its performance.

\citet{1} reference four datasets which are needed for this evaluation. RAKE requires a stop list of words to function, they evaluate RAKE using two stop lists, Fox and KA. RAKE is compared on two datasets, one about technical abstracts from a referenced study, and the MPQA Corpus. Upon inspection, the two stop lists are readily available (they provide KA, [4]). The MPQA dataset is also available. [5]

\subsection {Pseudo Code}
This is to do.

\section{Evaluation}

At a high level, we will be attempting to replicate Table 1.2 from \citeapos{1}'s paper. This means that we will be creating and testing 2 different keyword extracting methods, which are RAKE and TextRank. We will also be comparing the results of these two methods against the results published  results of \citeapos{hulth-2003-improved} published results as done by \citet{1}. 

This means that we will be comparing the number of extracted keywords from our implemented RAKE and TextRank methods against the number of correct keywords. For RAKE, this will include testing with a Keyword Adjacency stoplist, and Fox stoplist \citep{fox}. For TextRank, this will mean that we will be testing the co-occurence window of n=2 and 3.

After obtaining the number of correct keywords to extracted keyword, we will then calculate the precision, recall, and F-measure. We can then determine with these measures if we have replicate the results that RAKE has the highest precision and F-measure, but does not have the highest recall. 

\section{Remaining Work}

We have collected the referenced datasets and mapped out pseudo code, as mentioned above in our Implementation section, to obtain the key words of each abstract. We have also begun pre-processing our datasets to allow for keyword extraction. We still need to write the code based off of our pseudo code, extract the keywords from our data sets, evaluate the results from the keyword extraction, and discuss the results in our report. The schedule for our remaining work goes as follows:

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{3.3cm}|p{1.4cm}|p{1.7cm}|}
\hline \bf Task & \bf Target Date & \bf Person  Responsible \\  
\hline
 Create code to extract keywords & Nov 13 & All \\ 
\hline 
 Extract keywords from datasets & Nov 17 & All \\
\hline
 Evaluate results & Nov 24 & All \\
\hline
Complete report writing & Dec 6 & All  \\
\hline   
\end{tabular}
\end{center}
\end{table}

As for the distribution of work currently completed, Shouyang has created the code for our pre-processing of our data sets. Sharif summarized the papers in the Related Works section. Sharon formatted this paper. We all had a hand in writing this draft of the report. 

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{report}
\bibliographystyle{acl_natbib}




\end{document}
